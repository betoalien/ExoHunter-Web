# ExoHunter â€“ Web Application

**ExoHunter** is a Python + Flask + JavaScript project for analyzing exoplanet datasets (e.g., Kepler cumulative KOI files). It standardizes data, computes orbital/physical metrics, classifies candidates, and prepares feature sets for Machine Learning (ML). The app can be used both for scientific-style classification (*Confirmed, Candidate, False Positive*) and for unique anomaly detection categories (*Anomalous Signal*, *No Result/Invalid Data*).

---

## ğŸš€ Features

- Upload local CSV/Parquet files or load from cloud buckets.  
- Detect and normalize dataset columns to the KOI standard.  
- Automatically choose **Pandas** (small files) or **PySpark** (large files).  
- Compute derived metrics: semi-major axis, equilibrium temperature, transit depth, insolation, etc.  
- Classify each object into categories (Confirmed Planet, Candidate, Rocky, Gas Giant, Hot Jupiter, Habitable Zone Candidate, False Positive, Anomalous Signal, No Result).  
- Export processed results to CSV/JSON with classification, scores, and flags.  
- Manage labeling for ML training (human annotations, queues, and review).  
- Build and store feature vectors for anomaly detection and supervised ML.  
- Include Jupyter notebooks for exploration, baseline models, and supervised experiments.

---

## ğŸ“‚ Project Structure

```bash 
web_app/
â”œâ”€ app.py # Flask app entrypoint
â”œâ”€ config.py # Global configuration
â”œâ”€ docker-compose.yml
â”œâ”€ Dockerfile
â”œâ”€ .env / .env.example # Environment variables
â”‚
â”œâ”€ configs/ # YAML configs
â”‚ â”œâ”€ thresholds.yaml # Classification thresholds
â”‚ â”œâ”€ labeling.yaml # Labeling priorities
â”‚ â””â”€ mlflow.yaml # MLflow tracking
â”‚
â”œâ”€ data/ # Data lifecycle
â”‚ â”œâ”€ raw/ # Uploaded raw files
â”‚ â”œâ”€ interim/ # Normalized/intermediate data
â”‚ â”œâ”€ labeled/ # Human-labeled datasets
â”‚ â”œâ”€ uploads/ # User uploads
â”‚ â””â”€ outputs/ # Processed CSV/JSON reports
â”‚
â”œâ”€ feature_store/ # Versioned feature vectors
â”‚ â”œâ”€ v1/
â”‚ â””â”€ v2/
â”‚
â”œâ”€ ml/ # Machine Learning components
â”‚ â”œâ”€ artifacts/ # Models, scalers, encoders
â”‚ â”œâ”€ eval/ # Evaluation results, metrics, plots
â”‚ â”œâ”€ notebooks/ # Jupyter workflows
â”‚ â”œâ”€ pipelines/ # Dataset creation & training scripts
â”‚ â”œâ”€ feature_specs.yaml # Feature definitions
â”‚ â”œâ”€ splits.yaml # Train/val/test split rules
â”‚ â”œâ”€ data_card.md # Dataset documentation
â”‚ â”œâ”€ model_card.md # Model documentation
â”‚ â””â”€ README.md # ML-specific guide
â”‚
â”œâ”€ routes/ # Flask routes (API endpoints)
â”‚ â”œâ”€ upload.py # Upload files or bucket URLs
â”‚ â”œâ”€ process.py # Run pipeline (compute + classify)
â”‚ â”œâ”€ results.py # Fetch processed results
â”‚ â”œâ”€ labels.py # Label management endpoints
â”‚ â”œâ”€ features.py # Expose feature vectors
â”‚ â””â”€ health.py # Health check
â”‚
â”œâ”€ schemas/ # Data schemas and dictionaries
â”‚ â”œâ”€ expected_columns.json
â”‚ â””â”€ data_dictionary.md
â”‚
â”œâ”€ services/ # Core logic modules
â”‚ â”œâ”€ storage.py # File I/O (local/bucket)
â”‚ â”œâ”€ schema_detect.py # Column normalization
â”‚ â”œâ”€ engine_select.py # Pandas vs Spark choice
â”‚ â”œâ”€ load_dataframe.py # Safe data loading
â”‚ â”œâ”€ compute_metrics.py # Derived metrics (a, Teq, F, etc.)
â”‚ â”œâ”€ feature_builder.py # Build ML feature vectors
â”‚ â”œâ”€ classify.py # Classification rules + categories
â”‚ â”œâ”€ export_report.py # Generate CSV/JSON outputs
â”‚ â”œâ”€ labeling_queue.py # Manage items for labeling
â”‚ â””â”€ logging_utils.py # Logging helpers
â”‚
â”œâ”€ workers/ # Background processing
â”‚ â”œâ”€ spark_session.py
â”‚ â””â”€ tasks.py
â”‚
â”œâ”€ templates/ # HTML templates
â”‚ â”œâ”€ index.html
â”‚ â””â”€ results.html
â”‚
â”œâ”€ static/ # Frontend assets
â”‚ â”œâ”€ css/styles.css
â”‚ â””â”€ js/app.js
â”‚
â”œâ”€ tests/ # Unit tests
â”‚ â”œâ”€ test_schema_detect.py
â”‚ â”œâ”€ test_classify.py
â”‚ â””â”€ test_compute_metrics.py
â””â”€ ...
```

---

## ğŸ§ª Categories of Classification

- **confirmed_planet**  
- **candidate**  
- **likely_false_positive**  
- **rocky_candidate**  
- **super_earth_or_mini_neptune**  
- **hot_jupiter_candidate**  
- **habitable_zone_candidate**  
- **anomalous_signal** ğŸ”® (unique category for unusual or inconsistent data, possible exotic structures)  
- **no_result** âŒ (invalid or insufficient data)  

---

## âš™ï¸ Workflow

1. **Upload dataset** via `/api/upload` or UI.  
2. **Schema detection** maps columns to KOI standard.  
3. **Engine selection**: Pandas (small) or Spark (large).  
4. **Metrics computed** if missing (a, Teq, Insolation).  
5. **Classification** into categories with scores and flags.  
6. **Export report** (CSV/JSON).  
7. **Optional labeling**: push/review anomalies for ML.  
8. **Feature building**: store vectors in `feature_store/`.  
9. **ML training** (notebooks, pipelines).  

---

## ğŸ”® Machine Learning Prep

- Feature vectors stored in `feature_store/`.  
- Labels managed in `data/labeled/` with sources (human, heuristic, NASA disposition).  
- Baseline anomaly detection (Isolation Forest, LOF, etc.) in `ml/notebooks/03_baselines.ipynb`.  
- Supervised classifiers (XGBoost, LightGBM) in `ml/notebooks/04_supervised.ipynb`.  
- Model artifacts tracked in `ml/artifacts/` and documented in `ml/model_card.md`.  

---

## ğŸ› ï¸ Setup

```bash
# Clone and enter project
git clone https://github.com/betoalien/ExoHunter-Web.git
cd ExoHunter-Web

# Create virtual environment
python -m venv venv
source venv/bin/activate  # (Linux/Mac)
venv\Scripts\activate     # (Windows)

# Install dependencies
pip install -r requirements.txt

# Run Flask app
flask run


Optional:

Use Docker with docker-compose up.

Configure .env for storage backend (local, S3, GCS).

ğŸ“œ Notes

Input format: CSV (Kepler cumulative) or Parquet.

Use Parquet for large-scale data.

Classification thresholds adjustable in configs/thresholds.yaml.

Labeling priorities editable in configs/labeling.yaml.
```

------
## Testing
Test this project, visit https://exohunter-60bacde2a816.herokuapp.com/

------

## Contact me

For questions or contributions, open an issue or PR on GitHub.
Email: conect@albertocardenas.com
Website: https://www.albertocardenas.com
